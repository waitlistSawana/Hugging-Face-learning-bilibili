{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法的详细解释请看”./huggingface-tutorials-main/hg_03_pipeline.ipynb\n",
    "只要选择task，然后提供输入就可以实现任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6f0158897a4cceb3ce07bc4906ecbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HuangShengNan\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HuangShengNan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc949aa08d6f41e0b3905eb9982d46fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b22e842c844730b9f02b001cd91155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f76a5a9f0f4710831aad8076108e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dd79e8403043abbed5ed487c3a3f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.pipelines.text2text_generation.SummarizationPipeline'>\n"
     ]
    }
   ],
   "source": [
    "# 文本摘要的例子，直接使用默认模型，或者指定模型\n",
    "from transformers import pipeline\n",
    "\n",
    "# 配置处理器\n",
    "generator = pipeline(\"summarization\")\n",
    "print(generator.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 指定model，可以不运行\n",
    "# from transformers import pipeline\n",
    "\n",
    "# generator = pipeline(model=\"knkarthick/MEETING_SUMMARY\") # 不传入task也可以自动识别task类型\n",
    "# print(generator.__class__)  # 展示task的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' London is a 21st-century city with history stretching back to Roman times . At its centre stand the imposing Houses of Parliament, the iconic ‘Big Ben’ clock tower and Westminster Abbey . Across the Thames River, the London Eye observation wheel provides panoramic views of the city .'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "London, the capital of England and the United Kingdom, is a 21st-century city with history stretching back to Roman times. At its centre stand the imposing Houses of Parliament, the iconic ‘Big Ben’ clock tower and Westminster Abbey, site of British monarch coronations. Across the Thames River, the London Eye observation wheel provides panoramic views of the South Bank cultural complex, and the entire city.\n",
    "\"\"\"\n",
    "generator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另一个处理图片转文字的例子\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "print(classifier.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n",
    "classifier(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline有几个常用的重要参数：\n",
    "1. `device`\n",
    "    device参数接受以下几种可能的值:\n",
    "    - cpu：将模型加载到CPU上进行推理。\n",
    "    - cuda：将模型加载到第一块GPU进行推理，等同于'cuda:0'。\n",
    "    - cuda:X：将模型加载到编号为X的GPU上进行推理，X是GPU编号。\n",
    "    - mps：将模型加载到Mac OS上的Apple M1或M2芯片上利用Metal Performance Shaders (MPS) 进行推理。\n",
    "    - tpu：将模型加载到Tensor Processing Unit (TPU) 上进行推理,如Google Colab中的TPU。\n",
    "    - auto：自动选择一个可用的设备，会按优先级首先选择TPU，然后是GPU，最后是CPU。\n",
    "\n",
    "    在huggingface transformers的pipeline函数中，device参数也可以设置为一个整数，比如0，是为了将模型加载到特定编号的 `GPU` 进行推理。\n",
    "\n",
    "2. `batch_size`\n",
    "    默认情况下，pipeline并不会做批量推理（batch inference）。但是，pipeline函数依然提供了 `batch_size` 参数支持必要的batch inference。\n",
    "    `batch_size` 参数会影响到:\n",
    "    - 推理速度：增加batch_size通常可以提高GPU利用效率，加速推理速度。但若过大也会导致OOM。\n",
    "    - 内存占用：较大的batch_size会占用更多GPU显存。\n",
    "    - 推理效果：某些模型对batch_size比较敏感,大小不同会影响准确率。\n",
    "    - 支持长度：一些模型对最大输入长度有限制,batch_size过大可能超出模型支持长度。\n",
    "    - 输出：输出会是一个批量结果列表,如果只需要单条结果,需要索引获取。\n",
    "    \n",
    "    所以设置合适的batch_size需要综合考虑这些因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: auto\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' London is a 21st-century city with history stretching back to Roman times . At its centre stand the imposing Houses of Parliament, the iconic ‘Big Ben’ clock tower and Westminster Abbey . Across the Thames River, the London Eye observation wheel provides panoramic views of the city .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "London, the capital of England and the United Kingdom, is a 21st-century city with history stretching back to Roman times. At its centre stand the imposing Houses of Parliament, the iconic ‘Big Ben’ clock tower and Westminster Abbey, site of British monarch coronations. Across the Thames River, the London Eye observation wheel provides panoramic views of the South Bank cultural complex, and the entire city.\n",
    "\"\"\"\n",
    "generator(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
